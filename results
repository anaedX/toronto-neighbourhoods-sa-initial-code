
SVM: {accuracy: 0.6966}
NB:  {accuracy: 0.7449}
LR:  {accuracy: 0.6991}
RF:  {accuracy: 0.6786}


Naive Bayes Multinomial with 10 k-fold cross validation {'clf__alpha': 1, 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}:
              precision    recall  f1-score   support
           0     0.7247    0.7914    0.7566      4008
           1     0.7693    0.6982    0.7320      3993
    accuracy                         0.7449      8001
   macro avg     0.7470    0.7448    0.7443      8001
weighted avg     0.7470    0.7449    0.7443      8001


SVM with paramter hyper tuning (C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,
    probability=False, random_state=None, shrinking=True, tol=0.001,
    verbose=False):
              precision    recall  f1-score   support
           0     0.7083    0.6690    0.6879      18001
           1     0.6865    0.7243    0.7047      18001
    accuracy                         0.6966
   macro avg     0.6974    0.6966    0.6963      36002
weighted avg     0.6974    0.6966    0.6963      36002


Random Forest:
              precision    recall  f1-score   support
           0     0.6990    0.6351    0.6651       403
           1     0.6617    0.7223    0.6903       397
    accuracy                         0.6786       800
   macro avg     0.6803    0.6787    0.6776       800
weighted avg     0.6807    0.6786    0.6779       800


Logistic regeression with c=1, penalty=l2:
              precision    recall  f1-score   support
           0     0.7186    0.6572    0.6865     18050
           1     0.6826    0.7412    0.7107     17952
    accuracy                         0.6991     36002
   macro avg     0.7006    0.6992    0.6986     36002
weighted avg     0.7006    0.6991    0.6986     36002



Results:
Toronto:   {'Positive': 1184, 'Negative': 1209}
Montreal:  {'Positive': 1400, 'Negative': 1179}
Vancouver: {'Positive': 1348, 'Negative': 1122}